{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Automatants @ CentraleSupélec - CeSIA - Partie 4\n",
    "\n",
    "- Création : 02/2025 par [Nicolas Guillard](mailto:nicolas.guillar@securite-ia.fr) - bénévole au [CeSIA](https://www.securite-ia.fr/).\n",
    "- Dernière mise à jour : 05/03/2025\n",
    "\n",
    "Créer en s'inspirant particulièrement de [Générer des noms de villes et communes françaises](https://github.com/alxndrTL/villes) par [Alexandre TL](https://www.youtube.com/@alexandretl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du sujet et Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indications de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments de ce TP :\n",
    "- le présent carnet,\n",
    "  \n",
    "(et ceux qui seront installés grâce au script `Installation de l'environnement de travail`),\n",
    "- le répertoire `./utils` et les fichiers contenus,\n",
    "- le répertoire `./weights` contenant les poids des modèles utiles et ceux produits,\n",
    "- le répertoire `./images` contenant les illustrations des carnets,\n",
    "- le fichier de données `./villes.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de l'environnement de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script ci-dessous est destiné à installer les éléments nécessaires au fonctionnement de ce carnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "repo = \"workshop_cs_202503\"\n",
    "branch = \"main\"\n",
    "url_repo = f\"https://github.com/nicolasguillard/{repo}/archive/refs/heads/{branch}.zip\"\n",
    "target_dir = (\n",
    "  \"/content\"\n",
    "  if IN_COLAB\n",
    "  else \".\"\n",
    ")\n",
    "resources = [\"utils\", \"weights\", \"images\", \"villes.txt\"]\n",
    "\n",
    "if not Path(f\"{target_dir}/utils\").exists() :\n",
    "  print(\"=== Installation des ressources utiles à ce carnet ===\")\n",
    "  !wget -P {target_dir} {url_repo}\n",
    "  !unzip {target_dir}/{branch}.zip -d {target_dir}\n",
    "  for resource in resources:\n",
    "    !mv {target_dir}/{repo}-{branch}/{resource} {target_dir}/{resource}\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  !rm -f {target_dir}/{branch}.zip\n",
    "  print(\"=== Terminé ===\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    print(\"--- Rafraichissez au besoin la liste des fichiers à gauche si nécessaire ---\")\n",
    "else:\n",
    "  print(\"Il semble que des ressources nécessaires pour ce carnet soient déjà installés :\")\n",
    "  for resource in resources:\n",
    "    print(\"\\t\", f\"./{resource}\", \"présent\" if Path(f\"{target_dir}/{resource}\").exists else \"absent\")\n",
    "  print(\"Pour supprimer les ressources automatiquement installées, utilisez la fonction 'remove_resources()' dans un autre bloc de code.\")\n",
    "\n",
    "def remove_resources():\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  for resource in resources:\n",
    "    !rm -rf {target_dir}/{resource}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les modules et paramétrages globaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les modules nécessaires sont importés. A moins d'un besoin spécifique, il n'y aura pas besoin de modifier le bloc de code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules prédéfinis et tiers\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules créés pour le projet\n",
    "from utils import get_datasets, SOS, EOS, PAD, CityNameDataset\n",
    "from utils import load_transformer_model, TransformerConfig, LanguageModelForSAE, sample, CharTokenizer\n",
    "from utils import load_sae, AutoEncoder\n",
    "from utils import clean_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection du GPU selon l'environnement de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirer la limite du nombre maximal de lignes affichées dans un tableau pandas\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le thème de seaborn\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrer les graines aléatoires\n",
    "#pth_rnd_gen_device = torch.Generator(device).manual_seed(42)\n",
    "if device == \"cuda\":\n",
    "    pth_rnd_gen_device = torch.cuda.manual_seed(42)\n",
    "elif device == \"mps\":\n",
    "    pth_rnd_gen_device = torch.mps.manual_seed(42)\n",
    "pth_rnd_gen_cpu = torch.manual_seed(42)\n",
    "pth_rnd_gen = pth_rnd_gen_cpu if device == \"cpu\" else pth_rnd_gen_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, tokenizer, _ = get_datasets(\"./villes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement du modèle de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32 # dimension du modèle\n",
    "n_heads = 4 # nombre de têtes pour l'attention\n",
    "n_layers = 1 # nombre de couches\n",
    "dropout = 0.\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocabulary_size(),\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    max_len=max(train_dataset.max_len, test_dataset.max_len) - 1  # Because X and y : sequence[:-1] and sequence[1:] in dataset\n",
    ")\n",
    "\n",
    "filename = \"./weights/model_32__4_heads__1_layers.pth\" # A modifier selon le contexte\n",
    "#filename = \"`./weights/solutions/model_32__4_heads__1_layers.pth`\" # A décommenter selon le contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  target_dir_drive = '/content/drive/MyDrive'\n",
    "  if Path(f\"{target_dir_drive}/{repo}\").exists() :\n",
    "    filename_drive = filename.replace(\"/weights/\", f\"/{repo}/\")\n",
    "    !cp {target_dir_drive}/{filename_drive} {target_dir}/{filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_transformer_model(filename, class_model=LanguageModelForSAE, config=config, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement du SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_size = config.d_model\n",
    "num_features = 4 * config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./weights/sae_model_32__4_heads__1_layers.pth\" # A modifier selon le contexte\n",
    "#filename = \"./weights/solutions/sae_model_32__4_heads__1_layers.pt\" # A décommenter selon le contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  if Path(f\"{target_dir_drive}/{repo}\").exists() :\n",
    "    filename_drive = filename.replace(\"/weights/\", f\"/{repo}/\")\n",
    "    !cp {target_dir_drive}/{filename_drive} {target_dir}/{filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = load_sae(filename, act_size=act_size, num_features=num_features, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifier le comportement en le dirigeant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des valeurs maximum des activations de chaque caractéristique dans le SAE par rapport au jeu de données d'entrainement, afin de bénéficier d'un réfénrenciel pour y appliquer un facteur multiplicateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "sae.eval();\n",
    "\n",
    "#min_values_sae = torch.full((sae.num_features, 1), +float('inf'))\n",
    "max_values_sae = torch.full((sae.num_features, ), -float('inf'))\n",
    "\n",
    "#Pour chaque élément du jeu de données d'entrainement\n",
    "for X, _ in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "    X = X.to(device)\n",
    "    # récupération des activations cachées du Transformer\n",
    "    hidden_acts_transfo = model(X, act=True) # (B, S, d_model)\n",
    "    # récupération des activations de caractéristiques (features) correspondant\n",
    "    _, _, features, _, _ = sae(hidden_acts_transfo) # (B, S, n_features)\n",
    "    \n",
    "    features = features.to(\"cpu\")\n",
    "\n",
    "    max_features, _ = features.max(dim=0)\n",
    "    max_features, _ = max_features.max(dim=0)\n",
    "    max_values_sae = torch.max(max_values_sae, max_features)\n",
    "\n",
    "clean_memory(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de ces valeurs (valeur d'activation maximale par indice du neurone caché dans le SAE, sur le jeu de données):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max_values_sae)\n",
    "for i, v in enumerate(max_values_sae):\n",
    "    print(f\"{i:3d}: {v:.2f}\", end=\"   \" if (i+1)%11 else \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICES : génération contrôlée\n",
    "\n",
    "Il s'agit d'appliquer la méthode de contrôle consistant à modifier les valeurs d'activations des neurones cachés dans le SAE, en connaissant les concepts qu'ils représentent, selon ce que l'on en a interprété.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1savRqhjCV4b36dWcQcEPUxouS1m_iftK)\n",
    "\n",
    "![Contrôle avec SAE](./images/steering.png)\n",
    "\n",
    "On remarque que l'erreur correspondant à la différence entre l'activation et sa reconstruction est exploitée dans le calcul du tenseur reconstruit en tenant compte des valeurs de contrôle.\n",
    "\n",
    "Inspirez-vous largement du code de la fonction `sample()` de la partie 3 pour compléter cette fonction qui va générer des noms de commune en tenant compte des modifications qui sont founies via le tenseur `steering_vector`, contenant des lignes [`ìd neurone`, `modification`], selon la méthode illustrée dans le schéma ci-dessus.\n",
    "\n",
    "Si le `max_values_sae` est fourni en paramètre, la valeur `modification` sera un facteur multiplicatif de la valeur maximale prise par le neurone d'indice `ìd neurone` présente dans `max_values_sae`.\n",
    "\n",
    "> Conseil(s) : \n",
    "> - exploiter les méthodes d'indexation du type `steering_vector[:, 0]`, pour invoquer la première colonne d'un tenseur;\n",
    "> - ne pas oublié de bien calculer l'erreur;\n",
    "\n",
    "> NdA : vous constaterez après quelques tentatives d'utilisation de cette fonction que l'art de l'interprétabilité n'est pas une technique facilement maîtrisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steered_sample(\n",
    "        model: LanguageModelForSAE,\n",
    "        sae: AutoEncoder,\n",
    "        tokenizer: CharTokenizer,\n",
    "        steering_vector: torch.Tensor,\n",
    "        prompt: str = \"\",\n",
    "        max_values_sae: torch.Tensor = None,\n",
    "        device=\"cpu\",\n",
    "        g = torch.Generator(),\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - model (LanguageModelForSAE) :\n",
    "        - sae (AutoEncoder,) :\n",
    "        - tokenizer (CharTokenizer) :\n",
    "        - steering_vector (torch.Tensor) :\n",
    "        - prompt (str = \"\") :\n",
    "        - max_values_sae (torch.Tensor) :\n",
    "        - device (str) :\n",
    "        - g (torch.Generator) :\n",
    "    \"\"\"\n",
    "    ### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "    # DEBUT DE BLOC\n",
    "    \n",
    "    ### EXERCICE : à compléter\n",
    "\n",
    "    return None ### EXERCICE : remplacer None par les bonnes instructions\n",
    "    # DEBUT DE BLOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réalisons une génération dirigée de noms de commune, en imposant la valeur maximum du neurone ayant l'activation la plus forte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_activation_index = max_values_sae.argmax().item()\n",
    "\n",
    "steering_vector = torch.tensor([[max_activation_index, 1]], dtype=int, device=\"cpu\")\n",
    "\n",
    "for i in range(15):\n",
    "    print(steered_sample(\n",
    "        model,\n",
    "        sae,\n",
    "        tokenizer,\n",
    "        prompt=\"la\",\n",
    "        steering_vector=steering_vector.to(device),\n",
    "        max_values_sae=max_values_sae.to(device),\n",
    "        device=device,\n",
    "        g=pth_rnd_gen\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'opposé, réalisons une génération dirigée de noms de commune, en neutralisant ce neurone :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector = torch.tensor([[max_activation_index, 0]], dtype=int, device=\"cpu\")\n",
    "\n",
    "for i in range(15):\n",
    "    print(steered_sample(\n",
    "        model,\n",
    "        sae,\n",
    "        tokenizer,\n",
    "        prompt=\"la\",\n",
    "        steering_vector=steering_vector.to(device),\n",
    "        max_values_sae=max_values_sae.to(device),\n",
    "        device=device,\n",
    "        g=pth_rnd_gen\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparons avec une génération non dirigée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(sample(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=\"la\",\n",
    "        device=device,\n",
    "        g=pth_rnd_gen\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity_cnn_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
