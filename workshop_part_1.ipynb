{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop CentraleSupélec - CeSIA - Partie 1\n",
    "\n",
    "- Création : 02/2025 par [Nicolas Guillard](mailto:nicolas.guillar@securite-ia.fr) - bénévole au [CeSIA](https://www.securite-ia.fr/).\n",
    "\n",
    "Créé en adaptant et complétant le projet [Générer des noms de villes et communes françaises](https://github.com/alxndrTL/villes) par [Alexandre TL](https://www.youtube.com/@alexandretl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du sujet et Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments de ce TP :\n",
    "- le présent carnet,\n",
    "  \n",
    "(et ceux qui seront installés grâce au script `Installation de l'environnement de travail`),\n",
    "- le répertoire `./utils` et les fichiers contenus,\n",
    "- le répertoire `./weights` contenant les poids des modèles utiles et ceux produits,\n",
    "- le répertoire `./images` contenant les illustrations des carnets,\n",
    "- le fichier de données `./villes.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de l'environnement de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script ci-dessous est destiné à installer les éléments nécessaires au fonctionnement de ce carnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "repo = \"workshop_cs_202503\"\n",
    "branch = \"main\"\n",
    "url_repo = f\"https://github.com/nicolasguillard/{repo}/archive/refs/heads/{branch}.zip\"\n",
    "target_dir = (\n",
    "  \"/content\"\n",
    "  if IN_COLAB\n",
    "  else \".\"\n",
    ")\n",
    "resources = [\"utils\", \"weights\", \"images\", \"villes.txt\"]\n",
    "\n",
    "if not Path(f\"{target_dir}/utils\").exists() :\n",
    "  print(\"=== Installation des ressources utiles à ce carnet ===\")\n",
    "  !wget -P {target_dir} {url_repo}\n",
    "  !unzip {target_dir}/{branch}.zip -d {target_dir}\n",
    "  for resource in resources:\n",
    "    !mv {target_dir}/{repo}-{branch}/{resource} {target_dir}/{resource}\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  !rm -f {target_dir}/{branch}.zip\n",
    "  print(\"=== Terminé ===\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    print(\"--- Rafraichissez au besoin la liste des fichiers à gauche si nécessaire ---\")\n",
    "else:\n",
    "  print(\"Il semble que des ressources nécessaires pour ce carnet soient déjà installés :\")\n",
    "  for resource in resources:\n",
    "    print(\"\\t\", f\"./{resource}\", \"présent\" if Path(f\"{target_dir}/{resource}\").exists else \"absent\")\n",
    "  print(\"Pour supprimer les ressources automatiquement installées, utilisez la fonction 'remove_resources()' dans un autre bloc de code.\")\n",
    "\n",
    "def remove_resources():\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  for resource in resources:\n",
    "    !rm -rf {target_dir}/{resource}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les modules et paramétrages globaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les modules nécessaires sont importés. A moins d'un besoin spécifique, il n'y aura pas besoin de modifier le bloc de code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules prédéfinis et tiers\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection du GPU selon l'environnement de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirer la limite du nombre maximal de lignes affichées dans un tableau pandas\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le thème de seaborn\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrer les graines aléatoires\n",
    "#pth_rnd_gen_device = torch.Generator(device).manual_seed(42)\n",
    "if device == \"cuda\":\n",
    "    pth_rnd_gen_device = torch.cuda.manual_seed(42)\n",
    "elif device == \"mps\":\n",
    "    pth_rnd_gen_device = torch.mps.manual_seed(42)\n",
    "pth_rnd_gen_cpu = torch.manual_seed(42)\n",
    "pth_rnd_gen = pth_rnd_gen_cpu if device == \"cpu\" else pth_rnd_gen_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données\n",
    "\n",
    "Exécuter les différents blocs de code successivement afin de découvrir le jeu de données et son traitement destiné à produire les séquences pour le modèle de langgue basé sur un Transformer.\n",
    "\n",
    "Certains constats vous permettront d'apprécier les résultats obtenus dans les parties suivantes, en connaissant la référence \"vérité terrain\".\n",
    "\n",
    "Le jeu de données contient 36583 noms de communes françaises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"./villes.txt\", header=None, names=[\"nom\"])\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les premières informations structurelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution de la longueur des noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la longueur des chaînes de caractères dans la colonne \"nom\"\n",
    "df['length'] = df['nom'].apply(len)\n",
    "\n",
    "# Afficher la distribution de la longueur des chaînes de caractères\n",
    "length_distribution = df['length'].value_counts().sort_index()\n",
    "#print(length_distribution)\n",
    "\n",
    "# Afficher la distribution sous forme d'un histogramme\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=length_distribution.index, y=length_distribution.values, hue=length_distribution.values, palette=\"coolwarm\")\n",
    "plt.xlabel('Longueur des chaînes de caractères')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution de la longueur des chaînes de caractères')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fréquences des caractères dans les noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer toutes les chaînes de caractères de la colonne \"nom\"\n",
    "all_chars = ''.join(df['nom'])\n",
    "\n",
    "# Compter les occurrences de chaque caractère\n",
    "char_counts = Counter(all_chars)\n",
    "\n",
    "# Convertir le résultat en dataframe pour une meilleure lisibilité\n",
    "char_freq_df = pd.DataFrame(\n",
    "    char_counts.items(), columns=['Caractère', 'Fréquence']\n",
    "    ).sort_values(by='Fréquence', ascending=False)\n",
    "char_freq_df[\"Ratio Freq (%)\"] = char_freq_df[\"Fréquence\"] / char_freq_df[\"Fréquence\"].sum() * 100\n",
    "\n",
    "print(\"Nombre de caractères distincts :\", len(char_freq_df))\n",
    "display(char_freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taux de fréquence par rapport aux (dix premières) positions dans la chaîne de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiter la longueur des noms à 10 caractères\n",
    "df['nom_limited'] = df['nom'].str[:10]\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les fréquences des caractères par position\n",
    "position_char_freq = {i: Counter() for i in range(10)}\n",
    "\n",
    "# Remplir le dictionnaire avec les fréquences des caractères par position\n",
    "for name in df['nom_limited']:\n",
    "    for i, char in enumerate(name):\n",
    "        position_char_freq[i][char] += 1\n",
    "\n",
    "# Convertir le dictionnaire en dataframe pour une meilleure lisibilité\n",
    "position_char_freq_df = pd.DataFrame(position_char_freq).fillna(0).astype(int)\n",
    "\n",
    "# Limiter aux 15 premiers caractères les plus fréquents\n",
    "top_chars = position_char_freq_df.sum(axis=1).sort_values(ascending=False).head(15).index\n",
    "position_char_freq_df = position_char_freq_df.loc[top_chars]\n",
    "\n",
    "# Calculer le taux de présence par position\n",
    "position_char_rate_df = position_char_freq_df.div(position_char_freq_df.sum(axis=0), axis=1) * 100\n",
    "\n",
    "# Visualiser les taux de présence avec une carte de chaleur\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(position_char_rate_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Caractère')\n",
    "plt.title('Taux de présence des caractères par position (limité aux 10 premières positions)')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fréquence des composants de nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les chaînes de caractères par \"-\" ou \" \" et les concaténer\n",
    "element_separator = \"-' \"\n",
    "all_elements = ' '.join(df['nom'].str.replace(f\"[{element_separator}]\", \" \", regex=True).values).split()\n",
    "\n",
    "# Compter les occurrences de chaque élément\n",
    "element_counts = Counter(all_elements)\n",
    "\n",
    "# Convertir le résultat en dataframe pour une meilleure lisibilité\n",
    "element_freq_df = pd.DataFrame(element_counts.items(), columns=['Élément', 'Fréquence']).sort_values(by='Fréquence', ascending=False)\n",
    "\n",
    "print(f\"Nombre total de composants distincts : {len(element_freq_df)}\")\n",
    "n = 15\n",
    "print(f\"dont {n} exemples :\")\n",
    "display(element_freq_df.sample(n).sort_values(by='Fréquence', ascending=False))\n",
    "\n",
    "# Filtrer les éléments dont la fréquence est strictement supérieure à 1\n",
    "element_freq_sup_1_df = element_freq_df[element_freq_df['Fréquence'] > 1]\n",
    "\n",
    "# Afficher le nombre total d'éléments associés\n",
    "print(f\"Nombre total de composants présents plus d'une fois : {len(element_freq_sup_1_df)}\")\n",
    "n = 15\n",
    "print(f\"dont les {n} premiers :\")\n",
    "display(element_freq_sup_1_df.head(n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combien de noms de communes sont composées ?\n",
    "Quelle est la distribution du nombre de composants ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le nombre de composants pour chaque nom\n",
    "df['num_components'] = df['nom'].apply(lambda x: len([comp for comp in x if comp in element_separator]) + 1)\n",
    "\n",
    "# Afficher la distribution du taux de fréquence du nombre de composants\n",
    "component_distribution = df['num_components'].value_counts().sort_index()\n",
    "component_distribution_rate = component_distribution / len(df) * 100\n",
    "print(\"Distribution de la fréquence du nombre de composants (en valeur) :\")\n",
    "display(component_distribution.to_frame())\n",
    "\n",
    "# Afficher la distribution sous forme d'un histogramme\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=component_distribution_rate.index, y=component_distribution_rate.values, hue=component_distribution_rate.values, palette=\"coolwarm\")\n",
    "for i in range(len(component_distribution_rate)):\n",
    "    plt.text(i, component_distribution_rate.values[i] + 0.5, f'{component_distribution_rate.values[i]:.3f}%', ha='center')\n",
    "plt.xlabel('Nombre de composants')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution de la fréquence du nombre de composants (en %)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de la classe dérivée Dataset et des utilitaires associés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de la classe `CityNameDataset` fournissant les données pour les phases d'entrainement notamment, en créant une sous-classe dérivée de `torch.utils.data.Dataset`. Egalement création de la classe `CharTokenizer`.\n",
    "\n",
    "> NdA : code fourni à titre informatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au préalable, définition de jetons de contrôle de séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = \"<SOS>\" # Start Of Sequence\n",
    "EOS = \"<EOS>\" # End Of Sequence\n",
    "PAD = \"<PAD>\" # Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer():\n",
    "    def __init__(self, corpus: list[str]) -> None:\n",
    "        self.special_tokens =  [PAD, SOS, EOS] # PAD en premier pour indice 0\n",
    "\n",
    "         # liste des différents caractères distincts\n",
    "        chars = sorted(list(set(''.join(corpus))))\n",
    "        # ajout des trois jetons de contrôle\n",
    "        chars = self.special_tokens + chars\n",
    "        # tables de correspondances\n",
    "        self.char_to_int = {}\n",
    "        self.int_to_char = {}\n",
    "\n",
    "        # indexation des tables de correspondances\n",
    "        for (c, i) in tqdm(\n",
    "            zip(chars, range(len(chars))),\n",
    "            desc=\"creating vocabulary\",\n",
    "            total=len(chars)\n",
    "            ):\n",
    "            self.char_to_int[c] = i\n",
    "            self.int_to_char[i] = c\n",
    "\n",
    "    def __call__(self, string: str) -> list[int]:\n",
    "        return self.to_idx(string)\n",
    "            \n",
    "    def vocabulary_size(self) -> int:\n",
    "        return len(self.char_to_int)\n",
    "    \n",
    "    def to_idx(self, sequence: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Translate a sequence of chars to its conterparts of indexes in the vocabulary\n",
    "        \"\"\"\n",
    "        return [self.char_to_int[c] for c in sequence]\n",
    "    \n",
    "    def to_tokens(self, sequence: list[int]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Translate a sequence of indexes to its conterparts of chars in the vocabulary\n",
    "        \"\"\"\n",
    "        return [self.int_to_char[i] for i in sequence]\n",
    "    \n",
    "    def to_string(self, sequence: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Return the string corresponding to the sequence of indexes in the vocabulary\n",
    "        \"\"\"\n",
    "        return \"\".join([self.int_to_char[i] for i in sequence if i > 2])\n",
    "    \n",
    "\n",
    "\n",
    "class CityNameDataset(Dataset):\n",
    "    def __init__(self, names: list[str], tokenizer: CharTokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - names : collection of string\n",
    "            - vocabulary : maps of \"char to index\" and \"index to char\" based on names\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # création des séquences encodées\n",
    "        num_sequences = len(names)\n",
    "        self.max_len = max([len(name) for name in names]) + 2 # <SOS> et <EOS>\n",
    "        self.X = torch.zeros((num_sequences, self.max_len), dtype=torch.int32)\n",
    "        for i, name in tqdm(enumerate(names), total=num_sequences, desc=\"creatind dataset\"):\n",
    "            # encodage de la séquence : \"SOS s e q u e n c e EPS PAD PAD ... PAD\"\n",
    "            self.X[i] = torch.tensor(\n",
    "                self.tokenizer([SOS]) +\n",
    "                self.tokenizer(name) +\n",
    "                self.tokenizer([EOS]) +\n",
    "                self.tokenizer([PAD] * (self.max_len - len(name) - 2))\n",
    "            )\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.X[idx, :-1], self.X[idx, 1:]\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "        filename: str,\n",
    "        split_rate: float = 0.9\n",
    "        ) -> Tuple[CityNameDataset, CityNameDataset, CharTokenizer, int]:\n",
    "    \"\"\"\n",
    "    Return train and test datasets, and the max length in the processed string collection\n",
    "\n",
    "    Args:\n",
    "        - filename (str) : path and file name of string data\n",
    "        - split_rate (float) : rate of the split of the train data, in [0.; 1.]\n",
    "    \n",
    "    Returns:\n",
    "        - train_dataset, test_dataset, max_len (Tuple[CityNameDataset, CityNameDataset, int]) : \n",
    "            dataset for train, dataset for test, max length in the processed string collection\n",
    "    \"\"\"\n",
    "    # chargement des données\n",
    "    file = open(filename)\n",
    "    raws = file.read()\n",
    "    names = raws.replace('\\n', ',').split(',')\n",
    "    # mise à l'écart des villes avec un nom de longueur inférieure à 3\n",
    "    # et détermination de la plus grande longueur\n",
    "    names_ = []\n",
    "    max_len = 0\n",
    "    for n in names:\n",
    "        if len(names) > 2:\n",
    "            names_.append(n)\n",
    "            max_len = max(max_len, len(n))\n",
    "\n",
    "    # création du tokenizer\n",
    "    tokenizer = CharTokenizer(names_)\n",
    "\n",
    "    # mélange de l'ensemble des noms\n",
    "    names_ = random.sample(names_, len(names_))\n",
    "    # index de séparation train/val\n",
    "    n_split = int(split_rate * len(names_))\n",
    "    \n",
    "    train_dataset = CityNameDataset(names_[:n_split], tokenizer)\n",
    "    test_dataset = CityNameDataset(names_[n_split:], tokenizer)\n",
    "\n",
    "    return train_dataset, test_dataset, tokenizer, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests de la classe et des utilisataires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, tokenizer, max_len = get_datasets(\"./villes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longueur de la plus longue séquence d'un nom de commune\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Différents affichages de séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_dataset[1] # Get a dataset item (index = 1), the tuple X, y\n",
    "print(\"X (indexes):\", X)\n",
    "print(\"y (indexes) :\", y, end=\"\\n\\n\")\n",
    "\n",
    "print(\"X (chars):\", tokenizer.to_tokens(X.tolist()))\n",
    "print(\"y (chars):\", tokenizer.to_tokens(y.tolist()), end=\"\\n\\n\")\n",
    "\n",
    "print(\"X (string):\", tokenizer.to_string(X.tolist()))\n",
    "print(\"y (string):\", tokenizer.to_string(y.tolist()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de la classe `torch.utils.data.DataLoader` se basant sur un dataset de classe `CityNameDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print(\"Dimensions du batch\", batch_X.size(), end=\"\\n\\n\")\n",
    "\n",
    "X = batch_X[14]\n",
    "y = batch_y[14]\n",
    "\n",
    "print(\"X (indexes):\", X)\n",
    "print(\"y (indexes) :\", y, end=\"\\n\\n\")\n",
    "\n",
    "print(\"X (chars):\", tokenizer.to_tokens(X.tolist()))\n",
    "print(\"y (chars):\", tokenizer.to_tokens(y.tolist()), end=\"\\n\\n\")\n",
    "\n",
    "print(\"X (string):\", tokenizer.to_string(X.tolist()))\n",
    "print(\"y (string):\", tokenizer.to_string(y.tolist()), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity_cnn_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
