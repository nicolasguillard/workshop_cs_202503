{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Automatants @ CentraleSupélec - CeSIA - Partie 2\n",
    "\n",
    "- Création : 02/2025 par [Nicolas Guillard](mailto:nicolas.guillar@securite-ia.fr) - bénévole au [CeSIA](https://www.securite-ia.fr/).\n",
    "- Dernière mise à jour : 05/03/2025\n",
    "\n",
    "Créer en adaptant et complétant le projet [Générer des noms de villes et communes françaises](https://github.com/alxndrTL/villes) par [Alexandre TL](https://www.youtube.com/@alexandretl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du sujet et Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments de ce TP :\n",
    "- le présent carnet,\n",
    "  \n",
    "(et ceux qui seront installés grâce au script `Installation de l'environnement de travail`),\n",
    "- le répertoire `./utils` et les fichiers contenus,\n",
    "- le répertoire `./weights` contenant les poids des modèles utiles et ceux produits,\n",
    "- le répertoire `./images` contenant les illustrations des carnets,\n",
    "- le fichier de données `./villes.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de l'environnement de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script ci-dessous est destiné à installer les éléments nécessaires au fonctionnement de ce carnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "repo = \"workshop_cs_202503\"\n",
    "branch = \"main\"\n",
    "url_repo = f\"https://github.com/nicolasguillard/{repo}/archive/refs/heads/{branch}.zip\"\n",
    "target_dir = (\n",
    "  \"/content\"\n",
    "  if IN_COLAB\n",
    "  else \".\"\n",
    ")\n",
    "resources = [\"utils\", \"weights\", \"images\", \"villes.txt\"]\n",
    "\n",
    "if not Path(f\"{target_dir}/utils\").exists() :\n",
    "  print(\"=== Installation des ressources utiles à ce carnet ===\")\n",
    "  !wget -P {target_dir} {url_repo}\n",
    "  !unzip {target_dir}/{branch}.zip -d {target_dir}\n",
    "  for resource in resources:\n",
    "    !mv {target_dir}/{repo}-{branch}/{resource} {target_dir}/{resource}\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  !rm -f {target_dir}/{branch}.zip\n",
    "  print(\"=== Terminé ===\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    print(\"--- Rafraichissez au besoin la liste des fichiers à gauche si nécessaire ---\")\n",
    "else:\n",
    "  print(\"Il semble que des ressources nécessaires pour ce carnet soient déjà installés :\")\n",
    "  for resource in resources:\n",
    "    print(\"\\t\", f\"./{resource}\", \"présent\" if Path(f\"{target_dir}/{resource}\").exists else \"absent\")\n",
    "  print(\"Pour supprimer les ressources automatiquement installées, utilisez la fonction 'remove_resources()' dans un autre bloc de code.\")\n",
    "\n",
    "def remove_resources():\n",
    "  !rm -rf {target_dir}/{repo}-{branch}\n",
    "  for resource in resources:\n",
    "    !rm -rf {target_dir}/{resource}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les modules et paramétrages globaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les modules nécessaires sont importés. A moins d'un besoin spécifique, il n'y aura pas besoin de modifier le bloc de code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules prédéfinis et tiers\n",
    "import math\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules créés pour le projet\n",
    "from utils import get_datasets, SOS, EOS, PAD\n",
    "from utils import LanguageModel, CharTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection du GPU selon l'environnement de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirer la limite du nombre maximal de lignes affichées dans un tableau pandas\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le thème de seaborn\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrer les graines aléatoires\n",
    "#pth_rnd_gen_device = torch.Generator(device).manual_seed(42)\n",
    "if device == \"cuda\":\n",
    "    pth_rnd_gen_device = torch.cuda.manual_seed(42)\n",
    "elif device == \"mps\":\n",
    "    pth_rnd_gen_device = torch.mps.manual_seed(42)\n",
    "pth_rnd_gen_cpu = torch.manual_seed(42)\n",
    "pth_rnd_gen = pth_rnd_gen_cpu if device == \"cpu\" else pth_rnd_gen_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle de langue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est un Transformer inspiré de LLAMA 2. Il est composé d'un ensemble de modules dont le code des classes est fourni ci-dessous.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1Kag6BuRKdInm7TYHl0qIZiSmnKHz3mKS)\n",
    "\n",
    "![Modèle de langue](./images/language_model_details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules prédéfinis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vocab_size: int\n",
    "    d_model: int # D or d_model in comments\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    max_len: int # maximum sequence length (for positional embedding)\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "    norm_eps: float = 1e-5\n",
    "    super_attn: bool = False # overwrites flash to False\n",
    "    flash: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_heads == 0, \"d_model must be a multiple of n_heads\"\n",
    "\n",
    "        self.d_head = self.d_model // self.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            dim (int) : \n",
    "            eps (float) :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.query_proj = nn.Linear(config.d_model, config.d_model, bias=False) # d_query = n_heads*d_head as in the Transformer paper\n",
    "        self.key_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "\n",
    "        if not config.flash:\n",
    "            # compute the mask once and for all here \n",
    "            # registrer treats it like a parameter (device, state_dict...) without training\n",
    "            mask = torch.full((1, 1, config.max_len, config.max_len), float('-inf'))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer('mask', mask)\n",
    "\n",
    "        # LxL super attention params\n",
    "        if config.super_attn:\n",
    "            self.k_in_v_proj = nn.Linear(in_features=config.max_len, out_features=config.max_len, bias=False)\n",
    "\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.d_model, config.d_model, bias=config.bias)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            x (torch.Tensor) : input shaped (B, S, d_model)\n",
    "        \"\"\"\n",
    "        B, S, _ = x.size()\n",
    "\n",
    "        Q = self.query_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_query)\n",
    "        K = self.key_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_key)\n",
    "        V = self.value_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_head=d_value)\n",
    "\n",
    "        if self.config.flash and not self.config.super_attn:\n",
    "            attention = F.scaled_dot_product_attention(\n",
    "                Q, K, V, attn_mask=None, dropout_p=self.config.dropout if self.training else 0, is_causal=True\n",
    "                )\n",
    "        else:\n",
    "            QK_T = Q @ torch.transpose(K, 2, 3) # (B, n_heads, S, S)\n",
    "            QK_T = QK_T + self.mask[:, :, :S, :S]\n",
    "\n",
    "            attention_scores = torch.softmax(QK_T / math.sqrt(self.config.d_head), dim=3) # (B, n_heads, S, S)\n",
    "\n",
    "            if self.config.super_attn:\n",
    "                attention = self.attn_dropout(attention_scores) @ self.k_in_v_proj.weight @ V # (B, n_h, L, d_value=d_head)\n",
    "            else:\n",
    "                attention = self.attn_dropout(attention_scores) @ V # (B, n_h, S, d_value=d_head)\n",
    "\n",
    "        attention = attention.transpose(1, 2) # (B, S, n_heafs, d_head)\n",
    "        y = attention.contiguous().view(B, S, self.config.d_model) # n_heads * d_head = d_model\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) : configuration settings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
    "        self.fc_2 = nn.Linear(4 * config.d_model, config.d_model, bias=config.bias)\n",
    "        self.fc_3 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            x (torch.Tensor) : input data  shaped (B, S, d_model)\n",
    "        \"\"\"\n",
    "        x = self.dropout(self.fc_2(F.silu(self.fc_1(x)) * self.fc_3(x)))\n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.attention_norm = RMSNorm(config.d_model, config.norm_eps)\n",
    "        self.sa = SelfAttentionMultiHead(config)\n",
    "        self.mlp_norm = RMSNorm(config.d_model, config.norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "         Args :\n",
    "            x (torch.Tensor) : input data shaped (B, S, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.attention_norm(x))\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "\n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.PE = nn.Embedding(config.max_len, config.d_model)\n",
    "        self.in_dropout = nn.Dropout(config.dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, stop_at_layer: int = -1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            x (torch.Tensor) : input data shaped (B, S, d_model)\n",
    "            stop_at_layer (int) : return the ouput (activations) after the specified {layer}-th layer (1 -> n_layers)\n",
    "        \"\"\"\n",
    "        if stop_at_layer < 0:\n",
    "            stop_at_layer += len(self.layers) + 1\n",
    "        elif stop_at_layer == 0:\n",
    "            stop_at_layer = 1\n",
    "        assert stop_at_layer <= len(self.layers), \\\n",
    "            f\"stop_at_layer ({stop_at_layer}) should be in [{-len(self.layers)}, {len(self.layers)}]\"\n",
    "        _, S, _ = x.size()\n",
    "\n",
    "        # Add positional embedding\n",
    "        pos_emb = self.PE(torch.arange(0, S, dtype=torch.long, device=x.device))\n",
    "        x = self.in_dropout(x + pos_emb)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x) # (B, S, d_model)\n",
    "\n",
    "            if stop_at_layer == i+1:\n",
    "                return x\n",
    "        \n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE(S) : Définition de la classe du modèle de langue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de compléter le code de la classe du modèle `LanguageModel` ci-dessous, c'est-à-dire les méthodes `__init__()` et `forward()` en interprétant le schéma du modèle de langue fourni plus haut.\n",
    "\n",
    "> Conseil(s) :\n",
    "> - utilisation de la méthode `get_logits_()` dans `forward()`.\n",
    "> - `self.core` fait référence à la partie \"Transformer\" du modèle.\n",
    "> - `self.lm_head` correspond à \"Unembedding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, model_config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = model_config\n",
    "        self.embedding = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        \n",
    "        self.core = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        self.out_norm = None ### EXERCICE : remplacer None par les bonnes instructions\n",
    "        self.lm_head = None ### EXERCICE : remplacer None par les bonnes instructions\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self._init_normal()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_normal(self, module):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('fc_3.weight') or pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * self.config.n_layers))\n",
    "    \n",
    "    def get_logits_(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.out_norm(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            tokens (torch.Tensor) : input shaped (B, s, vocab_size) with s in [1; S]\n",
    "        \"\"\"\n",
    "\n",
    "        ### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "        # DEBUT DE BLOC\n",
    "\n",
    "        ### EXERCICE : à compléter\n",
    "\n",
    "        logits = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        # FIN DE BLOC\n",
    "        return logits #(B, S, vocab_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les hyperparamètres fournis pour la création du modèle et l'entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32 # dimension du modèle\n",
    "n_heads = 4 # nombre de têtes pour l'attention\n",
    "n_layers = 1 # nombre de couches\n",
    "dropout = 0.\n",
    "\n",
    "lr = 3e-4 # leraning rate\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 20\n",
    "print_each = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les jeux de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invocation des jeux de données pour l'entrainement et le test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, tokenizer, _ = get_datasets(\"./villes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le modèle de langue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation des paramètres et instantiation du modèle et de l'optimisateur.\n",
    "\n",
    "> NdA : à remarquer la valeur fournie au paramètre `max_len` pour l'instance de `TransformerConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocabulary_size(),\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    max_len=max(train_dataset.max_len, test_dataset.max_len) - 1  # Car X et y : sequence[:-1] et sequence[1:] retourné par les \"dataset“\n",
    "    )\n",
    "model = LanguageModel(config).to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"> Le modèle Transformer : \", model, sep=\"\\n\")\n",
    "print(f\"> Nombre de paramètres appris : {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boucle d'entrainement du modèle (cette phase prend moins de 2 minutes sur Colab avec un T4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Entrainement\"):\n",
    "    train_loss = 0\n",
    "    for X, y in tqdm(train_dataloader, total=len(train_dataloader), desc=f\"epoch #{epoch+1:2d} / {epochs}\"):\n",
    "        X = X.to(device) # (B, S)\n",
    "        y = y.to(device) # (B, S)\n",
    "\n",
    "        logits = model(X) # (B, S, vocab_size)\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1).long(),\n",
    "            ignore_index=tokenizer.char_to_int[PAD]\n",
    "            )\n",
    "        train_loss += loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for j, (X, y) in enumerate(test_dataloader):\n",
    "            X = X.to(device) # (B, S)\n",
    "            y = y.to(device) # (B, S)\n",
    "            logits = model(X) # (B, S, vocab_size)\n",
    "            val_loss += F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1).long(),\n",
    "                ignore_index=tokenizer.char_to_int[PAD]\n",
    "                ).item()\n",
    "        val_loss /= len(test_dataloader)\n",
    "        \n",
    "    print(f\"\\tperte entrainement: {loss.item():.2f} | perte de validation : {val_loss:.2f}\")\n",
    "    model.train()\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des poids du modèle\n",
    "\n",
    "> Conseil(s) : si vous souhaitez entrainer plusieurs fois le même modèle avec des hyperparamètres différentes, initialiser la variable `timestamp` ci-dessous à `True`. Cela provoquera l'ajout d'un marqueur temporel au format \"`YYYYMMDD-HHMMSS`\" dans le nom du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = False\n",
    "filename = f\"./weights/model_{d_model}__{n_heads}_heads__{n_layers}_layers\"\n",
    "if timestamp:\n",
    "    filename += \"__\" + datetime.datetime.now().strftime(\"%Y%M%d-%I%M%S\")\n",
    "filename += \".pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si dans l'environnement Colab, sauvegarde dans Google Drive pour utiliser le fichier sauvé dans un autre carnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  if not Path('/content/drive').exists():\n",
    "    drive.mount('/content/drive')\n",
    "  target_dir_drive = '/content/drive/MyDrive'\n",
    "  if not Path(f\"{target_dir_drive}/{repo}\").exists() :\n",
    "    !mkdir {target_dir_drive}/{repo}\n",
    "  filename_drive = filename.replace(\"/weights/\", f\"/{repo}/\")\n",
    "  !cp {target_dir}/{filename} {target_dir_drive}/{filename_drive}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des poids du modèle appris si nécassaires, pour éviter la phase d'apprentissage, et s'ils ont déjà été sauvés. Autrement, possibilité d'utiliser le fichier `./weights/solutions/model_32__4_heads__1_layers.pth` en remplacement si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_load_model = False # mettre à True four forcer le chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer_model(filename: str, config: TransformerConfig, device=\"cpu\", verbose: bool = True, class_model=LanguageModel):\n",
    "    model = class_model(config)\n",
    "    model.load_state_dict(\n",
    "        torch.load(filename, map_location=torch.device('cpu'), weights_only=True)\n",
    "        )\n",
    "    if verbose:\n",
    "        print(model)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_load_model:\n",
    "    filename = \"./weights/model_32__4_heads__1_layers.pth\" # A modifier selon le contexte\n",
    "    model = load_transformer_model(filename, config=config, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la perte sur l'ensemble (et non par batch) de chaque jeux de données à la fin de l'entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset.X[:, :-1].to(device) # (B, S)\n",
    "y = train_dataset.X[:, 1:].long().to(device) # (B, S)\n",
    "logits = model(X) # (B, S, vocab_size)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=tokenizer.char_to_int[PAD])\n",
    "print(f\"total train loss : {loss.item():.2f}\")\n",
    "\n",
    "X = test_dataset.X[:, :-1].to(device) # (B, S)\n",
    "y = test_dataset.X[:, 1:].long().to(device) # (B, S)\n",
    "logits = model(X) # (B, S, vocab_size)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=tokenizer.char_to_int[PAD])\n",
    "print(f\"total val loss   : {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devriez des valeurs inférieures ou autour de $1,80$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de noms de commune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de la fonction de génération des noms de communes, basée sur une méthode de probabilité (et non \"greedy\", ou \"beam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model: LanguageModel, tokenizer: CharTokenizer, prompt: str = \"\", device=\"cpu\", g = None) -> str:\n",
    "    idx = torch.tensor(\n",
    "        [tokenizer.char_to_int[SOS]] + tokenizer(prompt),\n",
    "        dtype=torch.int32,\n",
    "        device=device\n",
    "        ).unsqueeze(0)\n",
    "    \n",
    "    next_id = -1\n",
    "\n",
    "    while next_id != tokenizer.char_to_int[EOS]:\n",
    "        logits = model(idx) # (1, len_s, d_model)\n",
    "\n",
    "        # calcul des probas pour chaque élément du vocabulaire\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        # tirage au sort en prenant en compte ces probas\n",
    "        next_id = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # concaténation\n",
    "        idx = torch.cat([idx, torch.tensor(next_id, device=device).view(1, 1)], dim=1)\n",
    "\n",
    "        if idx.shape[1] > model.config.max_len:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.to_string(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE(S) : Génération de noms de commune\n",
    "\n",
    "Utiliser plusieurs fois la fonction `sample()` pour générer une vingtaine de noms de communes, en indiquant différentes valeurs (dont `\"\"`) de `prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "# DEBUT DE BLOC\n",
    "### EXERCICE : à compléter\n",
    "# FIN DE BLOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "# DEBUT DE BLOC\n",
    "### EXERCICE : à compléter\n",
    "# FIN DE BLOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE(S) : Etudes statistiques sur un jeu de donneés générées\n",
    "\n",
    "Retrouve-t-on les distributions constatées en explorant les données ? Générez une certain nombre de noms de communes et comparez.\n",
    "\n",
    "Les méthodes de calcul présentes dans le carnet de la première partie ont été factorisées sous forme de fonctions dans la librairie `utils` (plus exactement dans le fichier `./utils/data_explo.py`), et sont importées dans ce carnet par le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import freq_distribution, freq_char, rate_freq_by_position, freq_composition_element, composition_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec :\n",
    "- `freq_distribution()` retourne la distribution de la longueur des noms et retourne les valeurs;\n",
    "- `freq_char()` affiche le tableau des fréquences des caractères dans les noms;\n",
    "- `rate_freq_by_position()` affiche le graphique des taux de fréquence par rapport aux positions dans la chaîne de caractères;\n",
    "- `freq_composition_element()` affiche les tableaux de fréquences des composants de nom;\n",
    "- `composition_distribution()` affiche le tableau et le graphique de la distribution du nombre de composants de noms de communes et leur distribution.\n",
    "\n",
    "Parcourez le fichier `utils/data_explo.py` pour comprendre leur utilisation (c'est assez facile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générez 1000 noms de communes et constatez les différents résultats statistiques obtenus avec les mêmes méthodes que dans la partie 1.\n",
    "\n",
    "> *Conseils : manipulation d'un Dataframe Pandas qui contiendra les noms de communes générés et initialement stoqués dans une liste simple (Temps de génération sur Colab avec un T4 : autour de 2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération d'une liste de noms de commune\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "communes = []\n",
    "N = 1000\n",
    "\n",
    "### EXERCICE : à compléter\n",
    "\n",
    "df = pd.DataFrame(communes, columns=[\"nom\"])\n",
    "df[\"nom\"].describe()\n",
    "display(df[\"nom\"].sample(20))\n",
    "# FIN DE BLOC\n",
    "\n",
    "# Avec `freq_distribution()` : la distribution de la longueur des noms;\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "# FIN DE BLOC\n",
    "\n",
    "# Avec `freq_char()` : la fréquences des caractères dans les noms;\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "# FIN DE BLOC\n",
    "\n",
    "# Avec `rate_freq_by_position()` le taux de fréquence par rapport aux positions dans la chaîne de caractères;\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "# FIN DE BLOC\n",
    "\n",
    "# Avec `freq_composition_element()` la fréquence des composants de nom;\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "# FIN DE BLOC\n",
    "\n",
    "# Avec `composition_distribution()` le nombre de composant de noms de communes et leur distribution.\n",
    "### EXERCICE : compléter ce bloc avec les bonnes instructions\n",
    "# DEBUT DE BLOC\n",
    "# FIN DE BLOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles réflexions peut-on avoir au sujet de ce qu'a appris, ou n'a pas appris, le modèle ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity_cnn_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
