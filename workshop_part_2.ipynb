{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop CentraleSupélec - CeSIA\n",
    "\n",
    "- Création : 02/2025 par [Nicolas Guillard](mailto:nicolas.guillar@securite-ia.fr) - bénévole au [CeSIA](https://www.securite-ia.fr/).\n",
    "\n",
    "Créer en s'inspirant particulièrement de [Générer des noms de villes et communes françaises](https://github.com/alxndrTL/villes) par [Alexandre TL](https://www.youtube.com/@alexandretl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du sujet et Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indications de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments de ce TP :\n",
    "- le présent carnet\n",
    "- le répertoire `utils` et les fichiers contenus\n",
    "- le fichier de données\n",
    "- le répertoire weights contenant les poids des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les modules et paramétrages globaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les modules nécessaires sont importés. A moins d'un besoin spécifique, il n'y aura pas besoin de modifier le bloc de code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules prédéfinis et tiers\n",
    "import math\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules créés pour le projet\n",
    "from utils import CityNameDataset, SOS, EOS, PAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection du GPU selon l'environnement de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirer la limite du nombre maximal de lignes affichées dans un tableau pandas\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le thème de seaborn\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrer les graines aléatoires\n",
    "pth_rnd_gen = torch.Generator(device).manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explications et schémas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules prédéfinis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vocab_size: int\n",
    "    d_model: int # D or d_model in comments\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    max_len: int # maximum sequence length (for positional embedding)\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "    norm_eps: float = 1e-5\n",
    "    super_attn: bool = False # overwrites flash to False\n",
    "    flash: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_heads == 0, \"d_model must be a multiple of n_heads\"\n",
    "\n",
    "        self.d_head = self.d_model // self.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            dim (int) : \n",
    "            eps (float) :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) :\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.query_proj = nn.Linear(config.d_model, config.d_model, bias=False) # d_query = n_heads*d_head as in the Transformer paper\n",
    "        self.key_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "\n",
    "        if not config.flash:\n",
    "            # compute the mask once and for all here \n",
    "            # registrer treats it like a parameter (device, state_dict...) without training\n",
    "            mask = torch.full((1, 1, config.max_len, config.max_len), float('-inf'))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer('mask', mask)\n",
    "\n",
    "        # LxL super attention params\n",
    "        if config.super_attn:\n",
    "            self.k_in_v_proj = nn.Linear(in_features=config.max_len, out_features=config.max_len, bias=False)\n",
    "\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.d_model, config.d_model, bias=config.bias)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x : (B, S, d_model)\n",
    "\n",
    "        B, S, _ = x.size()\n",
    "\n",
    "        Q = self.query_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_query)\n",
    "        K = self.key_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_key)\n",
    "        V = self.value_proj(x).view(B, S, self.config.n_heads, self.config.d_head).transpose(1, 2) # (B, n_heads, S, d_head=d_value)\n",
    "\n",
    "        if self.config.flash and not self.config.super_attn:\n",
    "            attention = F.scaled_dot_product_attention(\n",
    "                Q, K, V, attn_mask=None, dropout_p=self.config.dropout if self.training else 0, is_causal=True\n",
    "                )\n",
    "        else:\n",
    "            QK_T = Q @ torch.transpose(K, 2, 3) # (B, n_heads, S, S)\n",
    "            QK_T = QK_T + self.mask[:, :, :S, :S]\n",
    "\n",
    "            attention_scores = torch.softmax(QK_T / math.sqrt(self.config.d_head), dim=3) # (B, n_heads, S, S)\n",
    "\n",
    "            if self.config.super_attn:\n",
    "                attention = self.attn_drop(attention_scores) @ self.k_in_v_proj.weight @ V # (B, n_h, L, d_value=d_head)\n",
    "            else:\n",
    "                attention = self.attn_drop(attention_scores) @ V # (B, n_h, S, d_value=d_head)\n",
    "\n",
    "        attention = attention.transpose(1, 2) # (B, S, n_heafs, d_head)\n",
    "        y = attention.contiguous().view(B, S, self.config.d_model) # n_heads * d_head = d_model\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) : configuration settings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
    "        self.fc_2 = nn.Linear(4 * config.d_model, config.d_model, bias=config.bias)\n",
    "        self.fc_3 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            x (torch.Tensor) : input data  shaped (B, S, d_model)\n",
    "        \"\"\"\n",
    "        x = self.dropout(self.fc_2(F.silu(self.fc_1(x)) * self.fc_3(x)))\n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            config (TransformerConfig) :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.attention_norm = RMSNorm(config.d_model, config.norm_eps)\n",
    "        self.sa = SelfAttentionMultiHead(config)\n",
    "        self.mlp_norm = RMSNorm(config.d_model, config.norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "         Args :\n",
    "            x (torch.Tensor) : input data shaped (B, S, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.attention_norm(x))\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "\n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.PE = nn.Embedding(config.max_len, config.d_model) \n",
    "        self.in_dropout = nn.Dropout(config.dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, stop_at_layer: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            x (torch.Tensor) : input data shaped (B, S, d_model)\n",
    "            stop_at_layer (int) : return the ouput (activations) after the specified {layer}-th layer (1 -> n_layers)\n",
    "        \"\"\"\n",
    "        _, S, _ = x.size()\n",
    "\n",
    "        # Add positional embedding\n",
    "        pos_emb = self.PE(torch.arange(0, S, dtype=torch.long, device=x.device))\n",
    "        x = self.in_dropout(x + pos_emb)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x) # (B, S, d_model)\n",
    "\n",
    "            if stop_at_layer == i+1:\n",
    "                return x\n",
    "        \n",
    "        return x # (B, S, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description du modèle `LanguageModel`\n",
    "\n",
    "Schéma (avec les indications de dimensionnalité)\n",
    "\n",
    "1/ structure\n",
    "- Embedding (`nn.Embedding`, avec `padding_idx=0`)\n",
    "- Transformer\n",
    "- normalisation\n",
    "- linear (`nn.Linear`) qui calcul les logits retournés par le modèle\n",
    "\n",
    "2/ génération par propagation\n",
    "utilisation de la méthode prédéfinie `get_logits_()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICE(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de définir la classe du modèle.\n",
    "Compléter les méthodes `__init__()` et `forward()` à partir des explications précédentes, en utilisant certains des modules prédéfinis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, model_config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = model_config\n",
    "        self.embedding = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        \n",
    "        self.core = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        self.out_norm = None ### EXERCICE : remplacer None par les bonnes instructions\n",
    "        self.lm_head = None ### EXERCICE : remplacer None par les bonnes instructions\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.apply(self._init_normal)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # taken from llama2.c\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_normal(self, module):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('fc_3.weight') or pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * self.config.n_layers))\n",
    "    \n",
    "    def get_logits_(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.out_norm(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            tokens (torch.Tensor) : input shaped (B, s, vocab_size) with s in [1; S]\n",
    "        \"\"\"\n",
    "\n",
    "        ### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "        # DEBUT DE BLOC\n",
    "        logits = None ### EXERCICE : remplacer None par les bonnes instructions \n",
    "        # FIN DE BLOC\n",
    "        return logits #(B, S, vocab_size)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset et Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CityNameDataset(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32 # dimension du modèle\n",
    "n_heads = 4 # nombre de têtes pour l'attention\n",
    "n_layers = 1 # nombre de couches\n",
    "dropout = 0.\n",
    "\n",
    "lr = 3e-4\n",
    "batch_size = 64\n",
    "\n",
    "iterations = 10000\n",
    "print_each = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(vocab_size=len(dataset.vocabulaire), d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout, max_len=dataset.max_len)\n",
    "model = LanguageModel(config).to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"> Le modèle Transformer : \", model, sep=\"\\n\")\n",
    "print(f\"> Nombre de paramètres : {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for i in trange(iterations):\n",
    "    x, Y = dataset.get_batch(split='train', batch_size=batch_size) # (B, S)\n",
    "    logits = model(x) # (B, S, vocab_size)\n",
    "\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=dataset.char_to_int[PAD])\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i%print_each==0:\n",
    "        model.eval()\n",
    "        x, Y = dataset.get_batch(batch_size=batch_size) # (B, S)\n",
    "        logits = model(x) # (B, S, vocab_size)\n",
    "        val_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=dataset.char_to_int[PAD]).item()\n",
    "\n",
    "        print(f\"\\tperte entrainement (itération #{i:5d}): {loss.item():.2f} | perte de validation : {val_loss:.2f}\")\n",
    "        model.train()\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.X_train[:, :-1].to(device) # (B, S)\n",
    "Y = dataset.X_train[:, 1:].long().to(device) # (B, S)\n",
    "logits = model(x) # (B, S, vocab_size)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=dataset.char_to_int['<pad>'])\n",
    "print(f\"total train loss : {loss.item():.2f}\")\n",
    "\n",
    "x = dataset.X_val[:, :-1].to(device) # (B, S)\n",
    "Y = dataset.X_val[:, 1:].long().to(device) # (B, S)\n",
    "logits = model(x) # (B, S, vocab_size)\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=dataset.char_to_int['<pad>'])\n",
    "print(f\"total val loss   : {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauver les poids du modèle.\n",
    "\n",
    "> Conseil : si vous souhaitez entrainer plusieurs fois le même modèle avec des hyperparamètres différentes, initialiser la variable `timestamp` ci-dessous à `True`. Cela provoquera l'ajout d'un marqueur temporel au format \"YYYYMMDD-HHMMSS\" dans le nom du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = False\n",
    "filename = f\"./weights/model_{d_model}__{n_heads}_heads__{n_layers}_layers\"\n",
    "if timestamp:\n",
    "    filename += \"__\" + datetime.datetime.now().strftime(\"%Y%M%d-%I%M%S\")\n",
    "filename += \".pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de noms de commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prompt = \"\", device=\"cpu\", g = torch.Generator(device)):\n",
    "    idx = torch.tensor([dataset.char_to_int[c] for c in prompt], dtype=torch.int32, device=device).unsqueeze(0)\n",
    "    idx = torch.cat([torch.tensor(dataset.char_to_int[SOS], device=device).view(1, 1), idx], dim=1)\n",
    "    next_id = -1\n",
    "\n",
    "    while next_id != dataset.char_to_int[EOS]:\n",
    "        logits = model(idx) # (1, len_s, d_model)\n",
    "\n",
    "        # calcul des probas pour chaque élément du vocabulaire\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        # tirage au sort en prenant en compte ces probas\n",
    "        next_id = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # concaténation\n",
    "        idx = torch.cat([idx, torch.tensor(next_id, device=device).view(1, 1)], dim=1)\n",
    "\n",
    "        if idx.shape[1] > config.max_len:\n",
    "            break\n",
    "        \n",
    "    return \"\".join([dataset.int_to_char[p.item()] for p in idx[0, 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE(S)\n",
    "\n",
    "Utiliser plusieurs fois la fonction `sample()` pour générer une vingtaine de noms de communes, en prenant différentes valeurs de `prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "# DEBUT DE BLOC\n",
    "\n",
    "# FIN DE BLOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCICE : compléter ce bloc avec les bonnes instructions \n",
    "# DEBUT DE BLOC\n",
    "\n",
    "# FIN DE BLOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE(S)\n",
    "\n",
    "Retrouve-t-on les distributions constatées en explorant les données ? Générez une certain nombre de noms de communes et comparer.\n",
    "\n",
    "Les méthodes de calcul présentes dans le carnet de la première partie ont été transférées sous forme de fonctions dans la labrairie `utils` (plus exactement le fichier `utils/data_explo.py`), et sont appelées dans le code ci-dessous. Elles concernent :\n",
    "- la distribution de la longueur des noms\n",
    "- la fréquences des caractères dans les noms\n",
    "- le taux de fréquence par rapport aux positions dans la chaîne de caractères\n",
    "- la fréquence des composants de nom\n",
    "- le nombre de composant de noms de communes et leur distribution\n",
    "\n",
    "Afin de les utiliser, parcourez le fichier `utils/data_explo.py` afin de comprendre leur utilisation (c'est très facile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import freq_distribution, freq_char, rate_freq_by_position, freq_composition_element, composition_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générer 1000 noms de communes et constater les différents résultats statistiques obtenues avec les mêmes méthodes que dans la partie 1.\n",
    "\n",
    "> *Conseils : création d'un Dataframe Pandas, après la génération des communes (Sur Colab avec un T4 : 2 minutes pour la génération)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
